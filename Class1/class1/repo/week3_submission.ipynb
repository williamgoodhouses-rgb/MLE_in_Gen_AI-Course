{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d2eb99",
   "metadata": {},
   "source": [
    "# Week 3 Homework – Voice Agent (ASR → LLM → TTS)\n",
    "\n",
    "This notebook contains my solutions for **Week 3**, where I build a small **voice assistant**:\n",
    "\n",
    "- Use **OpenAI audio models** for speech recognition (ASR) and text-to-speech (TTS).\n",
    "- Use **Ollama + Llama 3** as the local LLM “brain”.\n",
    "- Wrap everything in a **FastAPI** backend with a `/chat/` endpoint.\n",
    "- Maintain simple **multi-turn conversation memory** (5 turns).\n",
    "\n",
    "I developed and tested everything in my existing `ollama314` Conda environment on Windows 11.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9345d5a",
   "metadata": {},
   "source": [
    "## 1. Environment & Dependencies\n",
    "\n",
    "I use the same Conda environment as in Week 1 & 2 (`ollama314`):\n",
    "\n",
    "- Python 3.14\n",
    "- `openai` (already installed)\n",
    "- `fastapi`, `uvicorn`, `python-multipart` for serving the API\n",
    "\n",
    "If any of these are missing, I install them below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd7de78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 3 loaded successfully!\n",
      "OpenAI API Key detected: Yes\n",
      "OpenAI endpoint: https://api.openai.com/v1/\n",
      "Ollama endpoint: http://localhost:11434/v1/\n"
     ]
    }
   ],
   "source": [
    "# --- Load API Key from .env ---\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads OPENAI_API_KEY into environment\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Real OpenAI client (for ASR + TTS) ---\n",
    "openai_client = OpenAI()  \n",
    "# This automatically reads OPENAI_API_KEY from environment\n",
    "\n",
    "\n",
    "# --- Ollama local client (for LLM) ---\n",
    "ollama_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1/\",\n",
    "    api_key=\"ollama\"   # dummy key; Ollama ignores it\n",
    ")\n",
    "\n",
    "\n",
    "# --- Model configuration ---\n",
    "LLM_MODEL = \"llama3\"               # local LLM for responses\n",
    "ASR_MODEL = \"gpt-4o-transcribe\"    # or \"whisper-1\"\n",
    "TTS_MODEL = \"gpt-4o-mini-tts\"      # any available TTS model\n",
    "TTS_VOICE = \"alloy\"                # any supported voice\n",
    "\n",
    "\n",
    "print(\"Cell 3 loaded successfully!\")\n",
    "print(\"OpenAI API Key detected:\", \"Yes\" if os.getenv(\"OPENAI_API_KEY\") else \"No\")\n",
    "print(\"OpenAI endpoint:\", openai_client.base_url)\n",
    "print(\"Ollama endpoint:\", ollama_client.base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96f15e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.123.10-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting python-multipart\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting uvicorn[standard]\n",
      "  Using cached uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting starlette<0.51.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\willi\\anaconda3\\envs\\ollama314\\lib\\site-packages (from fastapi) (2.12.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\willi\\anaconda3\\envs\\ollama314\\lib\\site-packages (from fastapi) (4.15.0)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi)\n",
      "  Using cached annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\willi\\anaconda3\\envs\\ollama314\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\willi\\anaconda3\\envs\\ollama314\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\willi\\anaconda3\\envs\\ollama314\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\willi\\anaconda3\\envs\\ollama314\\lib\\site-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\willi\\anaconda3\\envs\\ollama314\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi) (3.11)\n",
      "Collecting click>=7.0 (from uvicorn[standard])\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\willi\\anaconda3\\envs\\ollama314\\lib\\site-packages (from uvicorn[standard]) (0.16.0)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\willi\\anaconda3\\envs\\ollama314\\lib\\site-packages (from uvicorn[standard]) (0.4.6)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard])\n",
      "  Downloading httptools-0.7.1-cp314-cp314-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard])\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\willi\\anaconda3\\envs\\ollama314\\lib\\site-packages (from uvicorn[standard]) (6.0.3)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard])\n",
      "  Downloading watchfiles-1.1.1-cp314-cp314-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard])\n",
      "  Downloading websockets-15.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading fastapi-0.123.10-py3-none-any.whl (111 kB)\n",
      "Downloading starlette-0.50.0-py3-none-any.whl (74 kB)\n",
      "Using cached uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading httptools-0.7.1-cp314-cp314-win_amd64.whl (88 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading watchfiles-1.1.1-cp314-cp314-win_amd64.whl (287 kB)\n",
      "Downloading websockets-15.0.1-py3-none-any.whl (169 kB)\n",
      "Installing collected packages: websockets, python-multipart, python-dotenv, httptools, click, annotated-doc, watchfiles, uvicorn, starlette, fastapi\n",
      "\n",
      "   ----------------------------------------  0/10 [websockets]\n",
      "   ----------------------------------------  0/10 [websockets]\n",
      "   -------- -------------------------------  2/10 [python-dotenv]\n",
      "   ---------------- -----------------------  4/10 [click]\n",
      "   ---------------- -----------------------  4/10 [click]\n",
      "   ---------------------------- -----------  7/10 [uvicorn]\n",
      "   ---------------------------- -----------  7/10 [uvicorn]\n",
      "   ---------------------------- -----------  7/10 [uvicorn]\n",
      "   -------------------------------- -------  8/10 [starlette]\n",
      "   ------------------------------------ ---  9/10 [fastapi]\n",
      "   ------------------------------------ ---  9/10 [fastapi]\n",
      "   ------------------------------------ ---  9/10 [fastapi]\n",
      "   ---------------------------------------- 10/10 [fastapi]\n",
      "\n",
      "Successfully installed annotated-doc-0.0.4 click-8.3.1 fastapi-0.123.10 httptools-0.7.1 python-dotenv-1.2.1 python-multipart-0.0.20 starlette-0.50.0 uvicorn-0.38.0 watchfiles-1.1.1 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Install / check dependencies (run once)\n",
    "\n",
    "%pip install --upgrade fastapi uvicorn[standard] python-multipart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3669fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.14.0 | packaged by Anaconda, Inc. | (main, Oct 22 2025, 08:58:42) [MSC v.1929 64 bit (AMD64)]\n",
      "OPENAI_API_KEY set: False\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Basic environment check\n",
    "\n",
    "import sys, os\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"OPENAI_API_KEY set:\", \"OPENAI_API_KEY\" in os.environ)\n",
    "\n",
    "# IMPORTANT:\n",
    "#  - OPENAI_API_KEY must be set in the environment for ASR + TTS.\n",
    "#  - Ollama must be running: `ollama serve` or Ollama app open.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89548a17",
   "metadata": {},
   "source": [
    "## 2. Clients & Configuration\n",
    "\n",
    "I use **two different “OpenAI-compatible” endpoints**:\n",
    "\n",
    "1. **Real OpenAI API** (internet)  \n",
    "   - For ASR (speech-to-text) and TTS (text-to-speech)  \n",
    "   - Uses `OPENAI_API_KEY`\n",
    "\n",
    "2. **Local Ollama (http://localhost:11434)**  \n",
    "   - For chat completions with `llama3`  \n",
    "   - Uses OpenAI-compatible API surface\n",
    "\n",
    "This keeps the “voice” models in the cloud but the **reasoning model local**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae09a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Real OpenAI for ASR + TTS\n",
    "openai_client = OpenAI()  # uses OPENAI_API_KEY\n",
    "\n",
    "# Ollama (local) for LLM\n",
    "ollama_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\",     # dummy string; Ollama ignores it\n",
    ")\n",
    "\n",
    "LLM_MODEL = \"llama3\"      # make sure this is already pulled in Ollama\n",
    "ASR_MODEL = \"gpt-4o-transcribe\"  # or \"whisper-1\"\n",
    "TTS_MODEL = \"gpt-4o-mini-tts\"    # any supported TTS model\n",
    "TTS_VOICE = \"alloy\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04557fdd",
   "metadata": {},
   "source": [
    "## 3. ASR – Audio → Text (OpenAI Audio)\n",
    "\n",
    "Here I implement a helper that accepts **raw audio bytes** and returns the\n",
    "transcribed text using OpenAI’s audio transcription API.\n",
    "\n",
    "The FastAPI endpoint will call this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fedb410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import NamedTemporaryFile\n",
    "from pathlib import Path\n",
    "\n",
    "def transcribe_audio_bytes(audio_bytes: bytes, suffix: str = \".wav\") -> str:\n",
    "    \"\"\"\n",
    "    Save uploaded audio bytes to a temporary file and send it\n",
    "    to OpenAI's audio transcription endpoint.\n",
    "    \"\"\"\n",
    "    with NamedTemporaryFile(delete=False, suffix=suffix) as tmp:\n",
    "        tmp.write(audio_bytes)\n",
    "        tmp_path = Path(tmp.name)\n",
    "\n",
    "    with tmp_path.open(\"rb\") as f:\n",
    "        transcript = openai_client.audio.transcriptions.create(\n",
    "            model=ASR_MODEL,\n",
    "            file=f,\n",
    "        )\n",
    "\n",
    "    # cleanup temp file\n",
    "    try:\n",
    "        tmp_path.unlink()\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    return transcript.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4afc2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ASR test – file not found: test.wav\n"
     ]
    }
   ],
   "source": [
    "# Optional sanity check: point this at a short WAV/MP3 file\n",
    "# If you don't have one handy you can skip this cell.\n",
    "\n",
    "TEST_AUDIO = \"test.wav\"  # change to your file\n",
    "\n",
    "if Path(TEST_AUDIO).exists():\n",
    "    with open(TEST_AUDIO, \"rb\") as f:\n",
    "        audio_bytes = f.read()\n",
    "    print(\"Transcription:\", transcribe_audio_bytes(audio_bytes))\n",
    "else:\n",
    "    print(\"Skipping ASR test – file not found:\", TEST_AUDIO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6ca25",
   "metadata": {},
   "source": [
    "## 4. LLM – Text → Text with Local Llama 3 (Ollama)\n",
    "\n",
    "Next, I build a wrapper around **Ollama’s OpenAI-compatible API**.\n",
    "\n",
    "I also add a very small **conversation memory** structure:\n",
    "\n",
    "- `history` is a list of `{user, bot}` turns.\n",
    "- For each new message, I:\n",
    "  - Add the last ≤5 turns as context\n",
    "  - Append the new user message\n",
    "  - Ask `llama3` for a reply.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "302f4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "ConversationTurn = Dict[str, str]\n",
    "history: List[ConversationTurn] = []  # global in-notebook memory\n",
    "\n",
    "\n",
    "def build_messages(user_text: str, history: List[ConversationTurn], max_turns: int = 5):\n",
    "    \"\"\"\n",
    "    Convert our simple history list into OpenAI-style chat messages.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a friendly, concise voice assistant. \"\n",
    "                \"Answer in short, clear sentences.\"\n",
    "            ),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Only keep the last max_turns turns\n",
    "    for turn in history[-max_turns:]:\n",
    "        messages.append({\"role\": \"user\", \"content\": turn[\"user\"]})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": turn[\"bot\"]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def generate_bot_reply(user_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Call local llama3 via Ollama to generate a reply.\n",
    "    \"\"\"\n",
    "    messages = build_messages(user_text, history)\n",
    "\n",
    "    response = ollama_client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    reply_text = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Update the conversation history\n",
    "    history.append({\"user\": user_text, \"bot\": reply_text})\n",
    "    # keep only recent turns\n",
    "    if len(history) > 10:\n",
    "        del history[:-10]\n",
    "\n",
    "    return reply_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "388e80f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a friendly voice assistant! I can help with various tasks, such as answering questions, providing definitions, giving directions, sending messages, setting reminders, and even telling jokes! What would you like to know or get done today?\n"
     ]
    }
   ],
   "source": [
    "test_reply = generate_bot_reply(\"Hi, who are you and what can you do?\")\n",
    "print(test_reply)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0285067c",
   "metadata": {},
   "source": [
    "## 5. TTS – Text → Audio (OpenAI TTS)\n",
    "\n",
    "Now I convert the assistant reply text into **spoken audio**.\n",
    "\n",
    "- I use OpenAI's `audio.speech` endpoint.\n",
    "- The helper returns the **path to an MP3 file** that can be served back\n",
    "  to clients (e.g. via FastAPI or directly downloaded).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88304aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai   # top-level module is still available alongside the client\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def text_to_speech_file(text: str, out_path: str = \"reply.mp3\") -> str:\n",
    "    \"\"\"\n",
    "    Use OpenAI TTS to synthesize speech from text.\n",
    "    The result is written to `out_path` and the path is returned.\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    out_path = Path(out_path)\n",
    "\n",
    "    with openai.audio.speech.with_streaming_response.create(\n",
    "        model=TTS_MODEL,\n",
    "        voice=TTS_VOICE,\n",
    "        input=text,\n",
    "    ) as response:\n",
    "        response.stream_to_file(out_path)\n",
    "\n",
    "    return str(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b87b7b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This will create an MP3 file in the current directory.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# You can play it with any media player.\u001b[39;00m\n\u001b[32m      4\u001b[39m TEST_SENTENCE = \u001b[33m\"\u001b[39m\u001b[33mHello, this is a small test of my Week 3 voice agent.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m mp3_path = \u001b[43mtext_to_speech_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST_SENTENCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtts_test.mp3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerated:\u001b[39m\u001b[33m\"\u001b[39m, mp3_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtext_to_speech_file\u001b[39m\u001b[34m(text, out_path)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m     10\u001b[39m out_path = Path(out_path)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspeech\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_streaming_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTTS_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvoice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTTS_VOICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(out_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ollama314\\Lib\\site-packages\\openai\\_response.py:626\u001b[39m, in \u001b[36mResponseContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _APIResponseT:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28mself\u001b[39m.__response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ollama314\\Lib\\site-packages\\openai\\resources\\audio\\speech.py:103\u001b[39m, in \u001b[36mSpeech.create\u001b[39m\u001b[34m(self, input, model, voice, instructions, response_format, speed, stream_format, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[33;03mGenerates audio from the input text.\u001b[39;00m\n\u001b[32m     69\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    100\u001b[39m \u001b[33;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    102\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mAccept\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/octet-stream\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/audio/speech\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvoice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspeed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspeech_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSpeechCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_legacy_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHttpxBinaryResponseContent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ollama314\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ollama314\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# This will create an MP3 file in the current directory.\n",
    "# You can play it with any media player.\n",
    "\n",
    "TEST_SENTENCE = \"Hello, this is a small test of my Week 3 voice agent.\"\n",
    "mp3_path = text_to_speech_file(TEST_SENTENCE, \"tts_test.mp3\")\n",
    "print(\"Generated:\", mp3_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca940ae2",
   "metadata": {},
   "source": [
    "## 6. FastAPI Integration – `/chat/` Endpoint\n",
    "\n",
    "Finally, I glue everything together in a **FastAPI app**:\n",
    "\n",
    "1. Client uploads an audio file to `/chat/` (`multipart/form-data`).\n",
    "2. Backend:\n",
    "   - Reads audio bytes.\n",
    "   - Calls `transcribe_audio_bytes` → `user_text`.\n",
    "   - Calls `generate_bot_reply(user_text)` → `bot_text`.\n",
    "   - Calls `text_to_speech_file(bot_text)` → `reply.mp3`.\n",
    "3. Returns a JSON response describing the interaction and the audio filename.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3ad3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import FileResponse, JSONResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "app = FastAPI(title=\"Week 3 Voice Agent\")\n",
    "\n",
    "# Allow simple local front-ends (HTML/JS) if needed\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "@app.post(\"/chat/\")\n",
    "async def chat_endpoint(file: UploadFile = File(...)):\n",
    "    \"\"\"\n",
    "    Main voice-chat endpoint.\n",
    "\n",
    "    1. Receive audio upload.\n",
    "    2. ASR → user text.\n",
    "    3. LLM → bot text.\n",
    "    4. TTS → MP3 reply.\n",
    "    \"\"\"\n",
    "    audio_bytes = await file.read()\n",
    "    user_text = transcribe_audio_bytes(audio_bytes, suffix=Path(file.filename).suffix or \".wav\")\n",
    "    bot_text = generate_bot_reply(user_text)\n",
    "\n",
    "    # Save audio reply as MP3 (overwrite each time for simplicity)\n",
    "    reply_audio_path = text_to_speech_file(bot_text, \"last_reply.mp3\")\n",
    "\n",
    "    return JSONResponse(\n",
    "        {\n",
    "            \"user_text\": user_text,\n",
    "            \"bot_text\": bot_text,\n",
    "            \"audio_file\": Path(reply_audio_path).name,\n",
    "            \"turns_in_memory\": len(history),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "@app.get(\"/audio/{filename}\")\n",
    "async def get_audio(filename: str):\n",
    "    \"\"\"\n",
    "    Simple endpoint to download / play the generated audio file.\n",
    "    \"\"\"\n",
    "    path = Path(filename)\n",
    "    if not path.exists():\n",
    "        return JSONResponse({\"error\": \"file not found\"}, status_code=404)\n",
    "    return FileResponse(path, media_type=\"audio/mpeg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76b987",
   "metadata": {},
   "source": [
    "## 7. Running the FastAPI Server\n",
    "\n",
    "To actually serve the voice agent, I run **uvicorn** from the terminal\n",
    "in the same directory as this notebook file.\n",
    "\n",
    "Example command:\n",
    "\n",
    "```bash\n",
    "uvicorn week3_submission:app --reload --port 8000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9487642",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Design Notes & Limitations\n",
    "\n",
    "- **ASR**: I chose `gpt-4o-transcribe` (or `whisper-1`) from OpenAI\n",
    "  instead of installing a heavy local Whisper model.  \n",
    "  This keeps my environment simpler and avoids GPU requirements.\n",
    "\n",
    "- **LLM**: I reuse my local **Ollama + Llama3** from Week 1–2 to provide\n",
    "  the conversational “brain”.  \n",
    "  The messages include up to **5 prior turns** for short-term memory.\n",
    "\n",
    "- **TTS**: I used OpenAI's TTS (`gpt-4o-mini-tts`) since installing\n",
    "  CozyVoice locally is heavier and more complex on Windows.\n",
    "\n",
    "- **FastAPI**: The app is intentionally small and modular:\n",
    "  - `transcribe_audio_bytes` – ASR\n",
    "  - `generate_bot_reply` – LLM + memory\n",
    "  - `text_to_speech_file` – TTS\n",
    "  - `chat_endpoint` – orchestration\n",
    "\n",
    "- **Security & performance**: For homework I keep everything simple\n",
    "  (no auth, no streaming). In a production system I would:\n",
    "  - Add authentication and rate limiting\n",
    "  - Stream ASR/LLM/TTS for lower latency\n",
    "  - Store conversation state in a database instead of in-memory list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15495791",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "In this Week 3 assignment I:\n",
    "\n",
    "1. **Implemented ASR** using OpenAI's audio transcription API.\n",
    "2. **Integrated a local LLM (Llama 3 via Ollama)** to act as the dialog agent.\n",
    "3. **Added TTS** using OpenAI's text-to-speech models.\n",
    "4. Combined everything in a **FastAPI microservice** with a `/chat/` endpoint.\n",
    "5. Implemented a basic **5-turn conversation memory** in Python.\n",
    "\n",
    "This notebook serves both as:\n",
    "- My **final submission** for Week 3, and\n",
    "- A starting point to extend into a richer voice assistant\n",
    "  (streaming, better memory, richer prompts, frontend UI, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccab92f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ollama314)",
   "language": "python",
   "name": "ollama314"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
