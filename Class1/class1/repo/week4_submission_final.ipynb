{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e9c443",
   "metadata": {},
   "source": [
    "# Week 4 ‚Äì Retrieval-Augmented Generation (RAG) with arXiv Papers\n",
    "\n",
    "This notebook implements a complete RAG-style retrieval system using a small collection of scientific PDFs.  \n",
    "The workflow includes:\n",
    "\n",
    "1. **PDF text extraction** using PyMuPDF  \n",
    "2. **Token-based sliding window chunking**  \n",
    "3. **Embedding generation** using `SentenceTransformer` (`all-MiniLM-L6-v2`)  \n",
    "4. **Vector index construction** using FAISS  \n",
    "5. **Semantic search demo** retrieving top-k relevant chunks  \n",
    "6. **FastAPI web service** exposing `/search` as an API endpoint  \n",
    "\n",
    "This notebook also saves:\n",
    "- `chunks.pkl`\n",
    "- `faiss.index`\n",
    "- `meta.pkl`\n",
    "\n",
    "These are loaded by `main.py` when serving results via FastAPI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e01c068",
   "metadata": {},
   "source": [
    "## 1. Environment and Dataset\n",
    "\n",
    "- Python environment: `ollama314` (Anaconda, Python 3.14)\n",
    "- Key libraries:\n",
    "  - `pymupdf` (imported as `fitz`) for PDF ‚Üí text\n",
    "  - `sentence-transformers` for embeddings (`all-MiniLM-L6-v2`)\n",
    "  - `faiss-cpu` for vector search (with a pure NumPy fallback if FAISS is unavailable)\n",
    "  - `fastapi` + `uvicorn` for the search API\n",
    "\n",
    "**Data assumption**\n",
    "\n",
    "- I have a folder of arXiv PDFs, for example:\n",
    "\n",
    "```text\n",
    "data/arxiv_pdfs/\n",
    "    paper_01.pdf\n",
    "    paper_02.pdf\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55697610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you see errors, install packages manually in your conda env.\n"
     ]
    }
   ],
   "source": [
    "# 2. (Optional) Install dependencies\n",
    "# Run this once if the packages are not installed in the environment.\n",
    "\n",
    "# !pip install -q \"sentence-transformers>=3.0.0\" pymupdf fastapi \"uvicorn[standard]\"\n",
    "# !pip install -q faiss-cpu   # may fail on some Python versions; we handle that later\n",
    "\n",
    "print(\"If you see errors, install packages manually in your conda env.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22a52b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\anaconda3\\envs\\rag311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF folder: D:\\AI study\\MLE_in_Gen_AI-Course\\week 0\\MLE_in_Gen_AI-Course\\class4\\data\\arxiv_pdfs\n",
      "FAISS available: True\n"
     ]
    }
   ],
   "source": [
    "# 3. Imports and configuration\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Try to import FAISS; if it fails, we'll use a simple NumPy-based fallback index.\n",
    "try:\n",
    "    import faiss\n",
    "    FAISS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    FAISS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è faiss-cpu not available ‚Äì will use a NumPy fallback index instead.\")\n",
    "\n",
    "# ----------------------\n",
    "# Paths and parameters\n",
    "# ----------------------\n",
    "PDF_FOLDER = Path(\"data/arxiv_pdfs\")         # folder containing the PDFs\n",
    "MAX_TOKENS = 512                             # chunk size (in whitespace tokens)\n",
    "OVERLAP = 50                                 # overlap between chunks\n",
    "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"        # sentence-transformers model\n",
    "TOP_K = 3                                    # top-k passages to retrieve\n",
    "\n",
    "print(\"PDF folder:\", PDF_FOLDER.resolve())\n",
    "print(\"FAISS available:\", FAISS_AVAILABLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b6f393",
   "metadata": {},
   "source": [
    "## üìÅ Files Saved in `data/index/`\n",
    "\n",
    "Running this notebook generates three important files:\n",
    "\n",
    "### **`chunks.pkl`**\n",
    "- List of all text chunks (strings)  \n",
    "- Used so the API can return the original text  \n",
    "\n",
    "### **`faiss.index`**\n",
    "- Vector index created by FAISS  \n",
    "- Stores embeddings for fast k-NN search  \n",
    "\n",
    "### **`meta.pkl`**\n",
    "- Metadata for each chunk:\n",
    "  - PDF filename\n",
    "  - chunk ID  \n",
    "\n",
    "These files are loaded directly by `main.py` so the FastAPI service works **without running the notebook again**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17eaba6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files.\n",
      "Sample PDF: Build_a_Large_Language_Model_(From_Scrat_v8_MEAP.pdf\n",
      "Preview (first 400 chars):\n",
      "\n",
      "MEAP Edition\n",
      "Manning Early Access Program\n",
      "Build a Large Language Model (From Scratch)\n",
      "Version 8\n",
      "Copyright 2024 Manning Publications\n",
      "For more information on this and other Manning titles go to manning.com.\n",
      "¬© Manning Publications Co. To comment go to liveBook\n",
      "Licensed to \u0000 \u0000 <149533107@qq.com>\n",
      "\n",
      "¬†\n",
      "welcome\n",
      "¬†\n",
      "Thank you for purchasing the MEAP edition of Build a Large Language Model (From\n",
      "Scratch).\n",
      "¬†\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "# 4. PDF ‚Üí text extraction (PyMuPDF)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Open a PDF and extract all text as a single string.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(str(pdf_path))\n",
    "    pages = []\n",
    "    for page in doc:\n",
    "        page_text = page.get_text()          # raw text from each page\n",
    "        # TODO (optional): clean page_text (remove headers/footers)\n",
    "        pages.append(page_text)\n",
    "    full_text = \"\\n\".join(pages)\n",
    "    return full_text\n",
    "\n",
    "\n",
    "# Quick smoke test (only if there is at least one PDF)\n",
    "sample_pdfs = sorted(PDF_FOLDER.glob(\"*.pdf\"))\n",
    "print(f\"Found {len(sample_pdfs)} PDF files.\")\n",
    "if sample_pdfs:\n",
    "    txt_preview = extract_text_from_pdf(sample_pdfs[0])\n",
    "    print(\"Sample PDF:\", sample_pdfs[0].name)\n",
    "    print(\"Preview (first 400 chars):\")\n",
    "    print(txt_preview[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1d28fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1898 small test chunks (64 tokens, overlap 16).\n",
      "First chunk preview:\n",
      "MEAP Edition Manning Early Access Program Build a Large Language Model (From Scratch) Version 8 Copyright 2024 Manning Publications For more information on this and other Manning titles go to manning.com. ¬© Manning Publications Co. To comment go to liveBook Licensed to \u0000 \u0000 <149533107@qq.com> welcome\n"
     ]
    }
   ],
   "source": [
    "# 5. Text chunking (sliding window over whitespace tokens)\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = MAX_TOKENS, overlap: int = OVERLAP) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a long text into overlapping chunks of up to max_tokens tokens.\n",
    "    Uses a simple whitespace tokenizer for clarity.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    step = max_tokens - overlap\n",
    "    for i in range(0, len(tokens), step):\n",
    "        chunk = tokens[i : i + max_tokens]\n",
    "        if not chunk:\n",
    "            break\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Quick test on the sample text\n",
    "if sample_pdfs:\n",
    "    test_chunks = chunk_text(txt_preview, max_tokens=64, overlap=16)\n",
    "    print(f\"Created {len(test_chunks)} small test chunks (64 tokens, overlap 16).\")\n",
    "    print(\"First chunk preview:\")\n",
    "    print(test_chunks[0][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6a4039d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build_a_Large_Language_Model_(From_Scrat_v8_MEAP.pdf: 198 chunks\n",
      "\n",
      "Total chunks in corpus: 198\n"
     ]
    }
   ],
   "source": [
    "# 6. Build corpus of chunks and metadata from all PDFs\n",
    "\n",
    "def build_corpus(pdf_folder: Path) -> Tuple[List[str], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Process all PDFs in the folder:\n",
    "      - extract full text\n",
    "      - chunk into segments\n",
    "    Returns:\n",
    "      chunks: list[str]\n",
    "      metadata: list[dict] (per chunk: pdf_name, chunk_id, etc.)\n",
    "    \"\"\"\n",
    "    all_chunks: List[str] = []\n",
    "    all_meta: List[Dict] = []\n",
    "\n",
    "    for pdf_path in sorted(pdf_folder.glob(\"*.pdf\")):\n",
    "        full_text = extract_text_from_pdf(pdf_path)\n",
    "        chunks = chunk_text(full_text, max_tokens=MAX_TOKENS, overlap=OVERLAP)\n",
    "\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            all_chunks.append(chunk)\n",
    "            all_meta.append({\n",
    "                \"pdf_name\": pdf_path.name,\n",
    "                \"chunk_id\": idx,\n",
    "            })\n",
    "\n",
    "        print(f\"{pdf_path.name}: {len(chunks)} chunks\")\n",
    "\n",
    "    print(f\"\\nTotal chunks in corpus: {len(all_chunks)}\")\n",
    "    return all_chunks, all_meta\n",
    "\n",
    "\n",
    "chunks, metadata = build_corpus(PDF_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c56c3ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\anaconda3\\envs\\rag311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\willi\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:08<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (198, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Embedding generation with Sentence-Transformers\n",
    "\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "print(f\"Loaded embedding model: {EMBED_MODEL_NAME}\")\n",
    "\n",
    "# Compute embeddings for all chunks\n",
    "embeddings = embed_model.encode(\n",
    "    chunks,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28a6fa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks, metadata, and index.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "DATA_DIR = Path(\"data/index\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save chunks\n",
    "with open(DATA_DIR / \"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n",
    "\n",
    "# Save metadata\n",
    "with open(DATA_DIR / \"meta.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "# Save FAISS index\n",
    "if FAISS_AVAILABLE:\n",
    "    faiss.write_index(index, str(DATA_DIR / \"index.faiss\"))\n",
    "else:\n",
    "    # Fallback: save numpy index\n",
    "    np.save(DATA_DIR / \"fallback_index.npy\", np.array(embeddings))\n",
    "\n",
    "print(\"Saved chunks, metadata, and index.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6332fd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FAISS IndexFlatL2.\n"
     ]
    }
   ],
   "source": [
    "# 8. Build vector index (FAISS if available, otherwise simple NumPy search)\n",
    "\n",
    "class SimpleNumpyIndex:\n",
    "    \"\"\"\n",
    "    Minimal FAISS-like interface using brute-force cosine similarity, for environments\n",
    "    where faiss-cpu is not available.\n",
    "    \"\"\"\n",
    "    def __init__(self, vectors: np.ndarray):\n",
    "        self.vectors = vectors / (np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-10)\n",
    "\n",
    "    def search(self, query_vectors: np.ndarray, k: int):\n",
    "        # query_vectors: shape (n_queries, dim)\n",
    "        query_norm = query_vectors / (np.linalg.norm(query_vectors, axis=1, keepdims=True) + 1e-10)\n",
    "        sims = query_norm @ self.vectors.T                # cosine similarity\n",
    "        # For compatibility with FAISS, we return distances (1 - sim)\n",
    "        indices = np.argsort(-sims, axis=1)[:, :k]\n",
    "        distances = 1.0 - np.take_along_axis(sims, indices, axis=1)\n",
    "        return distances.astype(\"float32\"), indices.astype(\"int64\")\n",
    "\n",
    "\n",
    "if FAISS_AVAILABLE:\n",
    "    dim = embeddings.shape[1]\n",
    "    faiss_index = faiss.IndexFlatL2(dim)\n",
    "    faiss_index.add(embeddings.astype(\"float32\"))\n",
    "    index = faiss_index\n",
    "    print(\"Using FAISS IndexFlatL2.\")\n",
    "else:\n",
    "    index = SimpleNumpyIndex(embeddings.astype(\"float32\"))\n",
    "    print(\"Using SimpleNumpyIndex (NumPy-based).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity checks for the built index and saved artifacts\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data/index\")\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "print(\"Number of metadata entries:\", len(metadata))\n",
    "\n",
    "assert embeddings.shape[0] == len(chunks) == len(metadata), \"Mismatch between embeddings, chunks, and metadata lengths.\"\n",
    "assert (DATA_DIR / \"faiss.index\").exists(), \"faiss.index file is missing.\"\n",
    "assert (DATA_DIR / \"chunks.pkl\").exists(), \"chunks.pkl file is missing.\"\n",
    "assert (DATA_DIR / \"meta.pkl\").exists(), \"meta.pkl file is missing.\"\n",
    "\n",
    "print(\"‚úÖ Sanity checks passed: index and artifacts look consistent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd9c48ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Semantic search helper function\n",
    "\n",
    "def search_chunks(query: str, k: int = TOP_K) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Embed the query, search the index, and return the top-k chunks with metadata.\n",
    "    \"\"\"\n",
    "    query_vec = embed_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    distances, indices = index.search(query_vec, k)\n",
    "    results = []\n",
    "    for rank, (dist, idx) in enumerate(zip(distances[0], indices[0]), start=1):\n",
    "        entry = {\n",
    "            \"rank\": rank,\n",
    "            \"distance\": float(dist),\n",
    "            \"pdf_name\": metadata[idx][\"pdf_name\"],\n",
    "            \"chunk_id\": metadata[idx][\"chunk_id\"],\n",
    "            \"text\": chunks[idx],\n",
    "        }\n",
    "        results.append(entry)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0fe3bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUERY: What is attention in neural networks?\n",
      "\n",
      "Rank 1 | pdf=Build_a_Large_Language_Model_(From_Scrat_v8_MEAP.pdf | chunk_id=32 | distance=0.9793\n",
      "----------------------------------------\n",
      "selected attention weights with dropout to reduce overfitting Stacking multiple causal attention modules into a multi-head attention module In the previous chapter, you learned how to prepare the input text for training LLMs. This involved splitting text into individual word and subword tokens, which can be encoded into vector representations, the so-called embeddings, for the LLM. In this chapter, we will now look at an integral part of the LLM architecture itself, attention mechanisms, as illustrated in Figure 3.1. 60 ¬© Manning Publications Co. To comment go to liveBook Licensed to \u0000 \u0000 <1495 ...\n",
      "\n",
      "Rank 2 | pdf=Build_a_Large_Language_Model_(From_Scrat_v8_MEAP.pdf | chunk_id=176 | distance=1.0050\n",
      "----------------------------------------\n",
      "neural networks to prevent overfitting by randomly dropping units (along with their connections) from the neural network during training: Dropout: A Simple Way to Prevent Neural Networks from Overfitting (2014) by Srivastava et al., https://jmlr.‚Äãorg/papers/‚Äãv15/srivastava14a.‚Äã html While using the multi-head attention based on scaled-dot product attention remains the most common variant of self-attention in practice, authors found that it's possible to also achieve good performance without the value weight matrix and projection layer: Simplifying Transformer Blocks (2023) by He and Hofmann, h ...\n",
      "\n",
      "Rank 3 | pdf=Build_a_Large_Language_Model_(From_Scrat_v8_MEAP.pdf | chunk_id=34 | distance=1.0110\n",
      "----------------------------------------\n",
      "RNNs, it is not essential to understand or study this architecture as we will not be using it in this book. The takeaway message of this section is that encoder-decoder RNNs had a shortcoming that motivated the design of attention mechanisms. 63 ¬© Manning Publications Co. To comment go to liveBook Licensed to \u0000 \u0000 <149533107@qq.com> 3.2 Capturing data dependencies with attention mechanisms Before transformer LLMs, it was common to use RNNs for language modeling tasks such as language translation, as mentioned previously. RNNs work fine for translating short sentences but don't work well for lon ...\n",
      "================================================================================\n",
      "QUERY: How do transformers handle long sequences?\n",
      "\n",
      "Rank 1 | pdf=Build_a_Large_Language_Model_(From_Scrat_v8_MEAP.pdf | chunk_id=6 | distance=1.1371\n",
      "----------------------------------------\n",
      "word at a time. Note that this figure shows the final stage of the translation process where the decoder has to generate only the final word (\"Beispiel\"), given the original input text (\"This is an example\") and a partially translated sentence (\"Das ist ein\"), to complete the translation. 8 ¬© Manning Publications Co. To comment go to liveBook Licensed to \u0000 \u0000 <149533107@qq.com> The transformer architecture depicted in Figure 1.4 consists of two submodules, an encoder and a decoder. The encoder module processes the input text and encodes it into a series of numerical representations or vectors t ...\n",
      "\n",
      "Rank 2 | pdf=Build_a_Large_Language_Model_(From_Scrat_v8_MEAP.pdf | chunk_id=10 | distance=1.2289\n",
      "----------------------------------------\n",
      "predicting text one word at a time, they are considered a type of autoregressive model. Autoregressive models incorporate their previous outputs as inputs for future predictions. Consequently, in GPT, each new word is chosen based on the sequence that precedes it, which improves coherence of the resulting text. Architectures such as GPT-3 are also significantly larger than the original transformer model. For instance, the original transformer repeated the encoder and decoder blocks six times. GPT-3 has 96 transformer layers and 175 billion parameters in total. 14 ¬© Manning Publications Co. To  ...\n",
      "\n",
      "Rank 3 | pdf=Build_a_Large_Language_Model_(From_Scrat_v8_MEAP.pdf | chunk_id=33 | distance=1.3099\n",
      "----------------------------------------\n",
      "translation process requires contextual understanding and grammar alignment. To address the issue that we cannot translate text word by word, it is common to use a deep neural network with two submodules, a so-called encoder and decoder. The job of the encoder is to first read in and process the entire text, and the decoder then produces the translated text. We already briefly discussed encoder-decoder networks when we introduced the transformer architecture in chapter 1 (section 1.4, Using LLMs for different tasks). Before the advent of transformers, recurrent neural networks (RNNs) were the  ...\n",
      "================================================================================\n",
      "QUERY: What are common evaluation metrics in NLP?\n",
      "\n",
      "Rank 1 | pdf=Build_a_Large_Language_Model_(From_Scrat_v8_MEAP.pdf | chunk_id=1 | distance=1.2379\n",
      "----------------------------------------\n",
      "the fundamental concepts behind large language models (LLMs) Insights into the transformer architecture from which LLMs, such as the ones used on the ChatGPT platform, are derived A plan for building an LLM from scratch Large language models (LLMs), such as those offered in OpenAI's ChatGPT, are deep neural network models that have been developed over the past few years. They ushered in a new era for Natural Language Processing (NLP). Before the advent of large language models, traditional methods excelled at categorization tasks such as email spam classification and straightforward pattern re ...\n",
      "\n",
      "Rank 2 | pdf=Build_a_Large_Language_Model_(From_Scrat_v8_MEAP.pdf | chunk_id=138 | distance=1.2670\n",
      "----------------------------------------\n",
      "type of cloud is typically associated with thunderstorms? Correct response: >> The type of cloud typically associated with thunderstorms is cumulonimbus. Model response: >> The type of cloud associated with thunderstorms is a cumulus cloud. ------------------------------------- Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Name the author of 'Pride and Prejudice'. Correct response: >> Jane Austen. Model response: >> The author of 'Pride and Prejudice' is Jane Austen. ------------------------------------- 282 ¬© Manning ...\n",
      "\n",
      "Rank 3 | pdf=Build_a_Large_Language_Model_(From_Scrat_v8_MEAP.pdf | chunk_id=86 | distance=1.2675\n",
      "----------------------------------------\n",
      "or tokens in the vocabulary to generate as the next token. In this section, we calculated the loss for two small text inputs for illustration purposes. In the next section, we apply the loss computation to the entire training and validation sets. 5.1.3 Calculating the training and validation set losses In this section, we first prepare the training and validation datasets that we will use to train the LLM later in this chapter. Then, we calculate the cross entropy for the training and validation sets, as illustrated in Figure 5.8, which is an important component of the model training process.  ...\n"
     ]
    }
   ],
   "source": [
    "# 10. Notebook demo: try a few sample queries\n",
    "\n",
    "sample_queries = [\n",
    "    \"What is attention in neural networks?\",\n",
    "    \"How do transformers handle long sequences?\",\n",
    "    \"What are common evaluation metrics in NLP?\",\n",
    "]\n",
    "\n",
    "for q in sample_queries:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"QUERY:\", q)\n",
    "    results = search_chunks(q, k=TOP_K)\n",
    "    for r in results:\n",
    "        print(f\"\\nRank {r['rank']} | pdf={r['pdf_name']} | chunk_id={r['chunk_id']} | distance={r['distance']:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "        print(r[\"text\"][:600], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd419778",
   "metadata": {},
   "source": [
    "## üöÄ Running the FastAPI Semantic Search Server\n",
    "\n",
    "After building the FAISS index and saving the chunk data, you can run a real web service using FastAPI.\n",
    "\n",
    "### **1. Activate the environment**\n",
    "```bash\n",
    "\n",
    "conda activate rag311\n",
    "\n",
    "### **2. Go to the Week 4 project folder\n",
    "\n",
    "cd \"D:/AI study/MLE_in_Gen_AI-Course/week 0/MLE_in_Gen_AI-Course/class4\"\n",
    "\n",
    "### **3. Start the API server\n",
    "\n",
    "uvicorn main:app --reload\n",
    "\n",
    "If successful, you will see output like:\n",
    "\n",
    "Uvicorn running on http://127.0.0.1:8000\n",
    "\n",
    "### **4. Open the API documentation\n",
    "\n",
    "Swagger UI ‚Üí http://127.0.0.1:8000/docs\n",
    "\n",
    "ReDoc ‚Üí http://127.0.0.1:8000/redoc\n",
    "\n",
    "You can run queries such as:\n",
    "\n",
    "What is PyTorch?\n",
    "\n",
    "What is attention in neural networks?\n",
    "\n",
    "How do transformers handle long sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c7e8f",
   "metadata": {},
   "source": [
    "## 2. FastAPI Retrieval Service\n",
    "\n",
    "Next, I expose the same retrieval logic via a small FastAPI app.\n",
    "\n",
    "The `/search` endpoint:\n",
    "\n",
    "- takes a query parameter `q` (the user question)\n",
    "- embeds `q`\n",
    "- searches the FAISS (or NumPy) index\n",
    "- returns the top-k chunks as JSON\n",
    "\n",
    "In a real project, this code would live in a separate `main.py` file and be run with:\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04b95d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. FastAPI app definition\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI(title=\"Week 4 RAG Search API\")\n",
    "\n",
    "\n",
    "class SearchResponseChunk(BaseModel):\n",
    "    rank: int\n",
    "    distance: float\n",
    "    pdf_name: str\n",
    "    chunk_id: int\n",
    "    text: str\n",
    "\n",
    "\n",
    "class SearchResponse(BaseModel):\n",
    "    query: str\n",
    "    results: List[SearchResponseChunk]\n",
    "\n",
    "\n",
    "@app.get(\"/search\", response_model=SearchResponse)\n",
    "async def search_endpoint(q: str, k: int = TOP_K):\n",
    "    \"\"\"\n",
    "    Receive a query `q`, embed it, retrieve top-k passages, and return them as JSON.\n",
    "    \"\"\"\n",
    "    results = search_chunks(q, k=k)\n",
    "    response_chunks = [SearchResponseChunk(**r) for r in results]\n",
    "    return SearchResponse(query=q, results=response_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c3cd9",
   "metadata": {},
   "source": [
    "### How to run the FastAPI app (outside the notebook)\n",
    "\n",
    "For the actual service, I would:\n",
    "\n",
    "1. Copy the relevant code (imports, model loading, index building, `search_chunks`, and the `FastAPI` app) into `main.py`.\n",
    "2. Make sure `embeddings`, `index`, `chunks`, and `metadata` are built at import time (or loaded from disk).\n",
    "3. Start the server:\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bcaae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from fastapi import FastAPI\n",
    "import numpy as np\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embeddings + chunks (make sure these files exist)\n",
    "DATA_DIR = Path(\"data/index\")\n",
    "\n",
    "with open(DATA_DIR / \"chunks.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)\n",
    "\n",
    "faiss_index = faiss.read_index(str(DATA_DIR / \"faiss.index\"))\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/search\")\n",
    "async def search(q: str, k: int = 3):\n",
    "    \"\"\"Return top-k matching chunks for query q.\"\"\"\n",
    "    query_vec = model.encode([q]).astype(\"float32\")\n",
    "    distances, indices = faiss_index.search(query_vec, k)\n",
    "    results = [chunks[i] for i in indices[0]]\n",
    "    return {\"query\": q, \"results\": results}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4c0d2",
   "metadata": {},
   "source": [
    "### How to run the FastAPI semantic search service (from the terminal)\n",
    "\n",
    "To expose the same search functionality as a web API, run these steps **outside the notebook**:\n",
    "\n",
    "1. Open a terminal and activate the Week 4 environment:\n",
    "\n",
    "   ```bash\n",
    "   conda activate rag311\n",
    "   ```\n",
    "\n",
    "2. Change into the Week 4 project folder (where `main.py` lives):\n",
    "\n",
    "   ```bash\n",
    "   cd \"D:/AI study/MLE_in_Gen_AI-Course/week 0/MLE_in_Gen_AI-Course/class4\"\n",
    "   ```\n",
    "\n",
    "3. Start the FastAPI app with Uvicorn:\n",
    "\n",
    "   ```bash\n",
    "   uvicorn main:app --reload\n",
    "   ```\n",
    "\n",
    "   - `main:app` means: import the `app` object from `main.py`  \n",
    "   - `--reload` watches for code changes and restarts the server automatically (handy during development).\n",
    "\n",
    "4. Open your browser at:\n",
    "\n",
    "   - `http://127.0.0.1:8000/docs`  ‚Üí interactive Swagger UI  \n",
    "   - `http://127.0.0.1:8000/search?q=your+question&k=3` ‚Üí direct JSON response\n",
    "\n",
    "5. When you are done testing, stop the server with **Ctrl + C** in the terminal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d718c34d",
   "metadata": {},
   "source": [
    "# 3. Reflection and Notes\n",
    "\n",
    "## ‚úÖ What I Implemented\n",
    "- A full RAG-style retrieval system over a PDF document dataset  \n",
    "- PDF ‚Üí text pipeline using PyMuPDF (`fitz`)  \n",
    "- Token-based sliding window chunking with adjustable sizes  \n",
    "- Dense vector embeddings via Sentence-Transformers (`all-MiniLM-L6-v2`)  \n",
    "- Fast vector search using FAISS (`IndexFlatL2`)  \n",
    "- Notebook-based semantic search demo  \n",
    "- A complete FastAPI `/search` endpoint returning JSON results  \n",
    "\n",
    "## üîß Potential Improvements\n",
    "- Try more powerful embedding models (e.g., `all-mpnet-base-v2`)  \n",
    "- Add richer metadata such as authors, year, or sections  \n",
    "- Add a cross-encoder (re-ranking step) for higher retrieval accuracy  \n",
    "- Integrate this retriever with an LLM (local via Ollama or via OpenAI)  \n",
    "- Expand the PDF dataset to multiple papers and build a larger index  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15885a31",
   "metadata": {},
   "source": [
    "# Appendix: Running Search Programmatically\n",
    "\n",
    "Example GET request:\n",
    "\n",
    "```bash\n",
    "curl -X GET \"http://127.0.0.1:8000/search?q=What%20is%20PyTorch?&k=3\" \\\n",
    "     -H \"accept: application/json\"\n",
    "Sample response:\n",
    "\n",
    "json\n",
    "Copy code\n",
    "{\n",
    "  \"query\": \"What is PyTorch?\",\n",
    "  \"results\": [\n",
    "    {\n",
    "      \"pdf\": \"...\",\n",
    "      \"chunk_id\": 32,\n",
    "      \"distance\": 0.9793,\n",
    "      \"text\": \"PyTorch is a tensor library...\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "yaml\n",
    "Copy code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a7606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag311)",
   "language": "python",
   "name": "rag311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
